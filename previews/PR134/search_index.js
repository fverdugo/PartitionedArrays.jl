var documenterSearchIndex = {"docs":
[{"location":"reference/ptimer/#Benchmarking","page":"Benchmarking","title":"Benchmarking","text":"","category":"section"},{"location":"reference/ptimer/#PTimer","page":"Benchmarking","title":"PTimer","text":"","category":"section"},{"location":"reference/ptimer/","page":"Benchmarking","title":"Benchmarking","text":"PTimer\nPTimer(a)","category":"page"},{"location":"reference/ptimer/#PartitionedArrays.PTimer","page":"Benchmarking","title":"PartitionedArrays.PTimer","text":"struct PTimer{...}\n\nType used to benchmark distributed applications based on PartitionedArrays.\n\nProperties\n\nProperties and type parameters are private\n\nSub-type hierarchy\n\nPTimer{...} <: Any\n\n\n\n\n\n","category":"type"},{"location":"reference/ptimer/#PartitionedArrays.PTimer-Tuple{Any}","page":"Benchmarking","title":"PartitionedArrays.PTimer","text":"PTimer(ranks;verbose::Bool=false)\n\nConstruct an instance of PTimer by using the same data distribution as for ranks.  If verbose==true, then a message will be printed each time a new section is added in the timer when calling toc!.\n\n\n\n\n\n","category":"method"},{"location":"reference/ptimer/#Measuring-elapsed-times","page":"Benchmarking","title":"Measuring elapsed times","text":"","category":"section"},{"location":"reference/ptimer/","page":"Benchmarking","title":"Benchmarking","text":"tic!\ntoc!","category":"page"},{"location":"reference/ptimer/#PartitionedArrays.tic!","page":"Benchmarking","title":"PartitionedArrays.tic!","text":"tic!(t::PTimer;barrier=false)\n\nReset the timer t to start measuring the time in a section.   If barrier==true, all process will be synchronized before  resetting the timer if using a distributed back-end. For MPI, this will result in a call to MPI.Barrier.\n\n\n\n\n\n","category":"function"},{"location":"reference/ptimer/#PartitionedArrays.toc!","page":"Benchmarking","title":"PartitionedArrays.toc!","text":"toc!(t::PTimer,name::String)\n\nFinish measuring a code section with name name in timer t.\n\n\n\n\n\n","category":"function"},{"location":"reference/ptimer/#Post-process","page":"Benchmarking","title":"Post-process","text":"","category":"section"},{"location":"reference/ptimer/","page":"Benchmarking","title":"Benchmarking","text":"statistics","category":"page"},{"location":"reference/ptimer/#PartitionedArrays.statistics","page":"Benchmarking","title":"PartitionedArrays.statistics","text":"statistics(t::PTimer)\n\nReturn a dictionary with statistics of the compute time for the sections currently stored in the timer t. \n\n\n\n\n\n","category":"function"},{"location":"reference/partition/#Data-partition","page":"Data partition","title":"Data partition","text":"","category":"section"},{"location":"reference/partition/#Partitioners","page":"Data partition","title":"Partitioners","text":"","category":"section"},{"location":"reference/partition/","page":"Data partition","title":"Data partition","text":"uniform_partition\nvariable_partition\ntrivial_partition\npartition_from_color","category":"page"},{"location":"reference/partition/#PartitionedArrays.uniform_partition","page":"Data partition","title":"PartitionedArrays.uniform_partition","text":"uniform_partition(ranks,np,n[,ghost[,periodic]])\n\nGenerate an N dimensional block partition of the indices in LinearIndices(np) with a (roughly) constant block size. The output is a vector of vectors containing the indices in each component of the partition. The eltype of the result implements the AbstractLocalIndices interface.\n\nArguments\n\nranks: Array containing the distribution of ranks.\nnp::NTuple{N}: Number of parts per direction.\nn::NTuple{N}: Number of global indices per direction.\nghost::NTuple{N}=ntuple(i->false,N): Use or not ghost indices per direction.\nperiodic::NTuple{N}=ntuple(i->false,N): Use or not periodic boundaries per direction.\n\nFor convenience, one can also provide scalar inputs instead tuples to create 1D block partitions. In this case, the argument np can be omitted and it will be computed as np=length(ranks).\n\nExamples\n\n2D partition of 4x4 indices into 2x2 parts without ghost\n\njulia> using PartitionedArrays\n\njulia> rank = LinearIndices((4,));\n\njulia> uniform_partition(rank,10)\n4-element Vector{PartitionedArrays.LocalIndicesWithConstantBlockSize{1}}:\n [1, 2]\n [3, 4]\n [5, 6, 7]\n [8, 9, 10]\n\njulia> uniform_partition(rank,(2,2),(4,4))\n4-element Vector{PartitionedArrays.LocalIndicesWithConstantBlockSize{2}}:\n [1, 2, 5, 6]\n [3, 4, 7, 8]\n [9, 10, 13, 14]\n [11, 12, 15, 16]\n\n\n\n\n\n","category":"function"},{"location":"reference/partition/#PartitionedArrays.variable_partition","page":"Data partition","title":"PartitionedArrays.variable_partition","text":"variable_partition(n_own,n_global[;start])\n\nBuild a 1D variable-size block partition of the range 1:n. The output is a vector of vectors containing the indices in each component of the partition. The eltype of the result implements the AbstractLocalIndices interface.\n\nArguments\n\nn_own::AbstractArray{<:Integer}: Array containing the block size for each part.\nn_global::Integer: Number of global indices. It should be equal to sum(n_own).\nstart::AbstractArray{Int}=scan(+,n_own,type=:exclusive,init=1): First global index in each part.\n\nWe ask the user to provide n_global and (optionally) start since discovering them requires communications.\n\nExamples\n\njulia> using PartitionedArrays\n\njulia> rank = LinearIndices((4,));\n\njulia> n_own = [3,2,2,3];\n\njulia> variable_partition(n_own,sum(n_own))\n4-element Vector{PartitionedArrays.LocalIndicesWithVariableBlockSize{1}}:\n [1, 2, 3]\n [4, 5]\n [6, 7]\n [8, 9, 10]\n\n\n\n\n\n","category":"function"},{"location":"reference/partition/#PartitionedArrays.trivial_partition","page":"Data partition","title":"PartitionedArrays.trivial_partition","text":"trivial_partition(ranks,n;destination=MAIN)\n\nwarning: Warning\nDocument me!\n\n\n\n\n\n","category":"function"},{"location":"reference/partition/#PartitionedArrays.partition_from_color","page":"Data partition","title":"PartitionedArrays.partition_from_color","text":"partition_from_color(ranks,global_to_color;multicast=false,source=MAIN)\n\nBuild an arbitrary 1d partition by defining the parts via the argument global_to_color (see below). The output is a vector of vectors containing the indices in each component of the partition. The eltype of the result implements the AbstractLocalIndices interface.\n\nArguments\n\nranks: Array containing the distribution of ranks.\nglobal_to_color: If multicast==false,  global_to_color[gid] contains the part id that owns the global id gid. If multicast==true, then   global_to_color[source][gid] contains the part id that owns the global id gid.\n\nKey-word arguments\n\nmulticast=false\nsource=MAIN\n\nThis function is useful when generating a partition using a graph partitioner such as METIS. The argument global_to_color is the usual output of such tools.\n\n\n\n\n\n","category":"function"},{"location":"reference/partition/#AbstractLocalIndices","page":"Data partition","title":"AbstractLocalIndices","text":"","category":"section"},{"location":"reference/partition/","page":"Data partition","title":"Data partition","text":"AbstractLocalIndices\nlocal_to_global\nown_to_global\nghost_to_global\nlocal_to_owner\nown_to_owner\nghost_to_owner\nglobal_to_local\nglobal_to_own\nglobal_to_ghost\nown_to_local\nghost_to_local\nlocal_to_own\nlocal_to_ghost\nlocal_length\nown_length\nghost_length\nglobal_length\npart_id","category":"page"},{"location":"reference/partition/#PartitionedArrays.AbstractLocalIndices","page":"Data partition","title":"PartitionedArrays.AbstractLocalIndices","text":"abstract type AbstractLocalIndices\n\nAbstract type representing the local, own, and ghost indices in a part of a partition of a range 1:n with length n.\n\nNotation\n\nLet 1:n be an integer range with length n. We denote the indices in 1:n as the  global indices. Let us consider a partition of 1:n. The indices in a part  in the partition are called the own indices of this part. I.e., each part owns a subset of 1:n. All these subsets are disjoint. Let us assume that each part is equipped with a second set of indices called the ghost indices.  The set of ghost indices in a given part is an arbitrary subset of the global indices 1:n that are owned by other parts. The union of the own and ghost indices is referred to as the local indices of this part. The sets of local indices might overlap between the different parts.\n\nThe sets of own, ghost, and local indices are stored using vector-like containers in concrete implementations of AbstractLocalIndices. This equips them with a certain order. The i-th own index in a part is defined as the one being stored at index i in the array that contains the own indices in this part (idem for ghost and local indices). The map between indices in these ordered index sets are given by functions such as local_to_global, own_to_local etc.\n\nSupertype hierarchy\n\nAbstractLocalIndices <: AbstractVector{Int}\n\n\n\n\n\n","category":"type"},{"location":"reference/partition/#PartitionedArrays.local_to_global","page":"Data partition","title":"PartitionedArrays.local_to_global","text":"local_to_global(indices)\n\nReturn an array with the global indices of the local indices in indices.\n\n\n\n\n\n","category":"function"},{"location":"reference/partition/#PartitionedArrays.own_to_global","page":"Data partition","title":"PartitionedArrays.own_to_global","text":"own_to_global(indices)\n\nReturn an array with the global indices of the own indices in indices.\n\n\n\n\n\n","category":"function"},{"location":"reference/partition/#PartitionedArrays.ghost_to_global","page":"Data partition","title":"PartitionedArrays.ghost_to_global","text":"ghost_to_global(indices)\n\nReturn an array with the global indices of the ghost indices in indices.\n\n\n\n\n\n","category":"function"},{"location":"reference/partition/#PartitionedArrays.local_to_owner","page":"Data partition","title":"PartitionedArrays.local_to_owner","text":"local_to_owner(indices)\n\nReturn an array with the owners of the local indices in indices.\n\n\n\n\n\n","category":"function"},{"location":"reference/partition/#PartitionedArrays.own_to_owner","page":"Data partition","title":"PartitionedArrays.own_to_owner","text":"own_to_owner(indices)\n\nReturn an array with the owners of the own indices in indices.\n\n\n\n\n\n","category":"function"},{"location":"reference/partition/#PartitionedArrays.ghost_to_owner","page":"Data partition","title":"PartitionedArrays.ghost_to_owner","text":"ghost_to_owner(indices)\n\nReturn an array with the owners of the ghost indices in indices.\n\n\n\n\n\n","category":"function"},{"location":"reference/partition/#PartitionedArrays.global_to_local","page":"Data partition","title":"PartitionedArrays.global_to_local","text":"global_to_local(indices)\n\nReturn an array with the inverse index map of local_to_global(indices).\n\n\n\n\n\n","category":"function"},{"location":"reference/partition/#PartitionedArrays.global_to_own","page":"Data partition","title":"PartitionedArrays.global_to_own","text":"global_to_own(indices)\n\nReturn an array with the inverse index map of own_to_global(indices).\n\n\n\n\n\n","category":"function"},{"location":"reference/partition/#PartitionedArrays.global_to_ghost","page":"Data partition","title":"PartitionedArrays.global_to_ghost","text":"global_to_ghost(indices)\n\nReturn an array with the inverse index map of ghost_to_global(indices).\n\n\n\n\n\n","category":"function"},{"location":"reference/partition/#PartitionedArrays.own_to_local","page":"Data partition","title":"PartitionedArrays.own_to_local","text":"own_to_local(indices)\n\nReturn an array with the local ids of the own indices in indices.\n\n\n\n\n\n","category":"function"},{"location":"reference/partition/#PartitionedArrays.ghost_to_local","page":"Data partition","title":"PartitionedArrays.ghost_to_local","text":"ghost_to_local(indices)\n\nReturn an array with the local ids of the ghost indices in indices.\n\n\n\n\n\n","category":"function"},{"location":"reference/partition/#PartitionedArrays.local_to_own","page":"Data partition","title":"PartitionedArrays.local_to_own","text":"local_to_own(indices)\n\nReturn an array with the inverse index map of own_to_local(indices).\n\n\n\n\n\n","category":"function"},{"location":"reference/partition/#PartitionedArrays.local_to_ghost","page":"Data partition","title":"PartitionedArrays.local_to_ghost","text":"local_to_ghost(indices)\n\nReturn an array with the inverse index map of ghost_to_local(indices).\n\n\n\n\n\n","category":"function"},{"location":"reference/partition/#PartitionedArrays.local_length","page":"Data partition","title":"PartitionedArrays.local_length","text":"local_length(indices)\n\nGet number of local ids in indices.\n\n\n\n\n\n","category":"function"},{"location":"reference/partition/#PartitionedArrays.own_length","page":"Data partition","title":"PartitionedArrays.own_length","text":"own_length(indices)\n\nGet number of own ids in indices.\n\n\n\n\n\n","category":"function"},{"location":"reference/partition/#PartitionedArrays.ghost_length","page":"Data partition","title":"PartitionedArrays.ghost_length","text":"ghost_length(indices)\n\nGet number of ghost ids in indices.\n\n\n\n\n\n","category":"function"},{"location":"reference/partition/#PartitionedArrays.global_length","page":"Data partition","title":"PartitionedArrays.global_length","text":"global_length(indices)\n\nGet number of global ids associated with indices.\n\n\n\n\n\n","category":"function"},{"location":"reference/partition/#PartitionedArrays.part_id","page":"Data partition","title":"PartitionedArrays.part_id","text":"part_id(indices)\n\nReturn the id of the part that is storing indices.\n\n\n\n\n\n","category":"function"},{"location":"reference/partition/#PRange","page":"Data partition","title":"PRange","text":"","category":"section"},{"location":"reference/partition/","page":"Data partition","title":"Data partition","text":"PRange\npartition(::PRange)","category":"page"},{"location":"reference/partition/#PartitionedArrays.PRange","page":"Data partition","title":"PartitionedArrays.PRange","text":"struct PRange{A}\n\nPRange (partitioned range) is a type representing a range of indices 1:n partitioned into several parts. This type is used to represent the axes of instances of PVector and PSparseMatrix.\n\nProperties\n\npartition::A\n\nThe item partition[i] is an object that contains information about the own, ghost, and local indices of part number i. typeof(partition[i]) is a type that implements the methods of the AbstractLocalIndices interface. Use this interface to access the underlying information about own, ghost, and local indices.\n\nSupertype hierarchy\n\nPRange{A} <: AbstractUnitRange{Int}\n\n\n\n\n\n","category":"type"},{"location":"reference/partition/#PartitionedArrays.partition-Tuple{PRange}","page":"Data partition","title":"PartitionedArrays.partition","text":"partition(a::PRange)\n\nGet a.partition.\n\n\n\n\n\n","category":"method"},{"location":"reference/arraymethods/#Array-methods","page":"Array methods","title":"Array methods","text":"","category":"section"},{"location":"reference/arraymethods/#Indices","page":"Array methods","title":"Indices","text":"","category":"section"},{"location":"reference/arraymethods/","page":"Array methods","title":"Array methods","text":"linear_indices\ncartesian_indices","category":"page"},{"location":"reference/arraymethods/#PartitionedArrays.linear_indices","page":"Array methods","title":"PartitionedArrays.linear_indices","text":"linear_indices(a)\n\nGenerate an array equal to LinearIndices(a), but whose type and can depend on a. Return LinearIndices(a) by default.\n\n\n\n\n\n","category":"function"},{"location":"reference/arraymethods/#PartitionedArrays.cartesian_indices","page":"Array methods","title":"PartitionedArrays.cartesian_indices","text":"cartesian_indices(a)\n\nGenerate an array equal to CartesianIndices(a), but whose type and can depend on a. Return CartesianIndices(a) by default.\n\n\n\n\n\n","category":"function"},{"location":"reference/arraymethods/#Transformations","page":"Array methods","title":"Transformations","text":"","category":"section"},{"location":"reference/arraymethods/","page":"Array methods","title":"Array methods","text":"MAIN\ni_am_main\nmap_main\ntuple_of_arrays","category":"page"},{"location":"reference/arraymethods/#PartitionedArrays.MAIN","page":"Array methods","title":"PartitionedArrays.MAIN","text":"const MAIN = 1\n\nConstant used to refer the main rank.\n\n\n\n\n\n","category":"constant"},{"location":"reference/arraymethods/#PartitionedArrays.i_am_main","page":"Array methods","title":"PartitionedArrays.i_am_main","text":"\n\n\n\n","category":"function"},{"location":"reference/arraymethods/#PartitionedArrays.map_main","page":"Array methods","title":"PartitionedArrays.map_main","text":"map_main(f,args...;kwargs...)\n\nLike map(f,args...) but only apply f to one component of the arrays in args.\n\nOptional key-word arguments\n\nmain = MAIN: The linear index of the component to map\notherwise = (args...)->nothing: The function to apply when mapping indices different from main.\n\nExamples\n\njulia> using PartitionedArrays\n\njulia> a = [1,2,3,4]\n4-element Vector{Int64}:\n 1\n 2\n 3\n 4\n\njulia> map_main(-,a,main=2)\n4-element Vector{Union{Nothing, Int64}}:\n   nothing\n -2\n   nothing\n   nothing\n\n\n\n\n\n","category":"function"},{"location":"reference/arraymethods/#PartitionedArrays.tuple_of_arrays","page":"Array methods","title":"PartitionedArrays.tuple_of_arrays","text":"tuple_of_arrays(a)\n\nConvert the array of tuples a into a tuple of arrays.\n\nExamples\n\njulia> using PartitionedArrays\n\njulia> a = [(1,2),(3,4),(5,6)]\n3-element Vector{Tuple{Int64, Int64}}:\n (1, 2)\n (3, 4)\n (5, 6)\n\njulia> b,c = tuple_of_arrays(a)\n([1, 3, 5], [2, 4, 6])\n\n\n\n\n\n","category":"function"},{"location":"reference/psparsematrix/#PSparseMatrix","page":"PSparseMatrix","title":"PSparseMatrix","text":"","category":"section"},{"location":"reference/psparsematrix/#Type-signature","page":"PSparseMatrix","title":"Type signature","text":"","category":"section"},{"location":"reference/psparsematrix/","page":"PSparseMatrix","title":"PSparseMatrix","text":"PSparseMatrix","category":"page"},{"location":"reference/psparsematrix/#PartitionedArrays.PSparseMatrix","page":"PSparseMatrix","title":"PartitionedArrays.PSparseMatrix","text":"struct PSparseMatrix{V,B,C,D,T}\n\nPSparseMatrix (partitioned sparse matrix) is a type representing a matrix whose rows are distributed (a.k.a. partitioned) over different parts for distributed-memory parallel computations. Each part stores a subset of the rows of the matrix and their corresponding non zero columns.\n\nThis type overloads numerous array-like operations with corresponding parallel implementations.\n\nProperties\n\nmatrix_partition::A\nrow_partition::B\ncol_partition::C\nassembled::Bool\n\nmatrix_partition[i] contains a (sparse) matrix with the local rows and the corresponding nonzero columns (the local columns) in the part number i. eltype(matrix_partition) == V. row_partition[i] and col_partition[i] contain information about the local, own, and ghost rows and columns respectively in part number i. The types eltype(row_partition) and eltype(col_partition) implement the AbstractLocalIndices interface. For assembled==true, it is assumed that the matrix data is fully contained in the own rows. \n\nSupertype hierarchy\n\nPSparseMatrix{V,A,B,C,T} <: AbstractMatrix{T}\n\nwith T=eltype(V).\n\n\n\n\n\n","category":"type"},{"location":"reference/psparsematrix/#Accessors","page":"PSparseMatrix","title":"Accessors","text":"","category":"section"},{"location":"reference/psparsematrix/","page":"PSparseMatrix","title":"PSparseMatrix","text":"local_values(::PSparseMatrix)\nown_own_values(::PSparseMatrix)\nown_ghost_values(::PSparseMatrix)\nghost_own_values(::PSparseMatrix)\nghost_ghost_values(::PSparseMatrix)","category":"page"},{"location":"reference/psparsematrix/#PartitionedArrays.local_values-Tuple{PSparseMatrix}","page":"PSparseMatrix","title":"PartitionedArrays.local_values","text":"local_values(a::PSparseMatrix)\n\nGet a vector of matrices containing the local rows and columns in each part of a.\n\nThe row and column indices of the returned matrices can be mapped to global indices, own indices, ghost indices, and owner by using local_to_global, local_to_own, local_to_ghost, and local_to_owner, respectively.\n\n\n\n\n\n","category":"method"},{"location":"reference/psparsematrix/#PartitionedArrays.own_own_values-Tuple{PSparseMatrix}","page":"PSparseMatrix","title":"PartitionedArrays.own_own_values","text":"own_own_values(a::PSparseMatrix)\n\nGet a vector of matrices containing the own rows and columns in each part of a.\n\nThe row and column indices of the returned matrices can be mapped to global indices, local indices, and owner by using own_to_global, own_to_local, and own_to_owner, respectively.\n\n\n\n\n\n","category":"method"},{"location":"reference/psparsematrix/#PartitionedArrays.own_ghost_values-Tuple{PSparseMatrix}","page":"PSparseMatrix","title":"PartitionedArrays.own_ghost_values","text":"own_ghost_values(a::PSparseMatrix)\n\nGet a vector of matrices containing the own rows and ghost columns in each part of a.\n\nThe row indices of the returned matrices can be mapped to global indices, local indices, and owner by using own_to_global, own_to_local, and own_to_owner, respectively.\n\nThe column indices of the returned matrices can be mapped to global indices, local indices, and owner by using ghost_to_global, ghost_to_local, and ghost_to_owner, respectively.\n\n\n\n\n\n","category":"method"},{"location":"reference/psparsematrix/#PartitionedArrays.ghost_own_values-Tuple{PSparseMatrix}","page":"PSparseMatrix","title":"PartitionedArrays.ghost_own_values","text":"ghost_own_values(a::PSparseMatrix)\n\nGet a vector of matrices containing the ghost rows and own columns in each part of a.\n\nThe row indices of the returned matrices can be mapped to global indices, local indices, and owner by using ghost_to_global, ghost_to_local, and ghost_to_owner, respectively.\n\nThe column indices of the returned matrices can be mapped to global indices, local indices, and owner by using own_to_global, own_to_local, and own_to_owner, respectively.\n\n\n\n\n\n","category":"method"},{"location":"reference/psparsematrix/#PartitionedArrays.ghost_ghost_values-Tuple{PSparseMatrix}","page":"PSparseMatrix","title":"PartitionedArrays.ghost_ghost_values","text":"ghost_ghost_values(a::PSparseMatrix)\n\nGet a vector of matrices containing the ghost rows and columns in each part of a.\n\nThe row and column indices of the returned matrices can be mapped to global indices, local indices, and owner by using ghost_to_global, ghost_to_local, and ghost_to_owner, respectively.\n\n\n\n\n\n","category":"method"},{"location":"reference/psparsematrix/#Constructors","page":"PSparseMatrix","title":"Constructors","text":"","category":"section"},{"location":"reference/psparsematrix/","page":"PSparseMatrix","title":"PSparseMatrix","text":"PSparseMatrix(a,b,c,d)\npsparse(f,b,c)\npsparse(f,a,b,c,d,e)\npsparse!\npsystem\npsystem!","category":"page"},{"location":"reference/psparsematrix/#PartitionedArrays.PSparseMatrix-NTuple{4, Any}","page":"PSparseMatrix","title":"PartitionedArrays.PSparseMatrix","text":"PSparseMatrix(matrix_partition,row_partition,col_partition,assembled)\n\nBuild an instance for PSparseMatrix from the underlying fields matrix_partition, row_partition, col_partition, assembled.\n\n\n\n\n\n","category":"method"},{"location":"reference/psparsematrix/#PartitionedArrays.psparse-Tuple{Any, Any, Any}","page":"PSparseMatrix","title":"PartitionedArrays.psparse","text":"psparse(f,row_partition,col_partition;assembled)\n\nBuild an instance of PSparseMatrix from the initialization function f and the partition for rows and columns row_partition and col_partition.\n\nEquivalent to\n\nmatrix_partition = map(f,row_partition,col_partition)\nPSparseMatrix(matrix_partition,row_partition,col_partition,assembled)\n\n\n\n\n\n","category":"method"},{"location":"reference/psparsematrix/#PartitionedArrays.psparse-NTuple{6, Any}","page":"PSparseMatrix","title":"PartitionedArrays.psparse","text":"psparse([f,]I,J,V,row_partition,col_partition;kwargs...) -> Task\n\nCrate an instance of PSparseMatrix by setting arbitrary entries from each of the underlying parts. It returns a task that produces the instance of PSparseMatrix allowing latency hiding while performing the communications needed in its setup.\n\n\n\n\n\n","category":"method"},{"location":"reference/psparsematrix/#PartitionedArrays.psparse!","page":"PSparseMatrix","title":"PartitionedArrays.psparse!","text":"psparse!(C::PSparseMatrix,V,cache)\n\n\n\n\n\n","category":"function"},{"location":"reference/psparsematrix/#PartitionedArrays.psystem","page":"PSparseMatrix","title":"PartitionedArrays.psystem","text":"psystem(I,J,V,I2,V2,rows,cols;kwargs...)\n\n\n\n\n\n","category":"function"},{"location":"reference/psparsematrix/#PartitionedArrays.psystem!","page":"PSparseMatrix","title":"PartitionedArrays.psystem!","text":"psystem!(A,b,V,V2,cache)\n\n\n\n\n\n","category":"function"},{"location":"reference/psparsematrix/#Assembly","page":"PSparseMatrix","title":"Assembly","text":"","category":"section"},{"location":"reference/psparsematrix/","page":"PSparseMatrix","title":"PSparseMatrix","text":"assemble(::PSparseMatrix,rows)\nassemble!(::PSparseMatrix,::PSparseMatrix,cache)\nconsistent(::PSparseMatrix,rows)\nconsistent!(::PSparseMatrix,::PSparseMatrix,cache)","category":"page"},{"location":"reference/psparsematrix/#PartitionedArrays.assemble-Tuple{PSparseMatrix, Any}","page":"PSparseMatrix","title":"PartitionedArrays.assemble","text":"assemble(A::PSparseMatrix[,rows];kwargs...)\n\n\n\n\n\n","category":"method"},{"location":"reference/psparsematrix/#PartitionedArrays.assemble!-Tuple{PSparseMatrix, PSparseMatrix, Any}","page":"PSparseMatrix","title":"PartitionedArrays.assemble!","text":"assemble!(B::PSparseMatrix,A::PSparseMatrix,cache)\n\n\n\n\n\n","category":"method"},{"location":"reference/psparsematrix/#PartitionedArrays.consistent-Tuple{PSparseMatrix, Any}","page":"PSparseMatrix","title":"PartitionedArrays.consistent","text":"consistent(A::PSparseMatrix,rows;kwargs...)\n\n\n\n\n\n","category":"method"},{"location":"reference/psparsematrix/#PartitionedArrays.consistent!-Tuple{PSparseMatrix, PSparseMatrix, Any}","page":"PSparseMatrix","title":"PartitionedArrays.consistent!","text":"consistent!(B::PSparseMatrix,A::PSparseMatrix,cache)\n\n\n\n\n\n","category":"method"},{"location":"reference/psparsematrix/#Re-partition","page":"PSparseMatrix","title":"Re-partition","text":"","category":"section"},{"location":"reference/psparsematrix/","page":"PSparseMatrix","title":"PSparseMatrix","text":"repartition(::PSparseMatrix,rows,cols)\nrepartition!(::PSparseMatrix,::PSparseMatrix,cache)\nrepartition(::PSparseMatrix,::PVector,rows,cols)\nrepartition!(::PSparseMatrix,::PVector,::PSparseMatrix,::PVector,cache)","category":"page"},{"location":"reference/psparsematrix/#PartitionedArrays.repartition-Tuple{PSparseMatrix, Any, Any}","page":"PSparseMatrix","title":"PartitionedArrays.repartition","text":"repartition(A::PSparseMatrix,new_rows,new_cols;reuse=false)\n\n\n\n\n\n","category":"method"},{"location":"reference/psparsematrix/#PartitionedArrays.repartition!-Tuple{PSparseMatrix, PSparseMatrix, Any}","page":"PSparseMatrix","title":"PartitionedArrays.repartition!","text":"repartition!(B::PSparseMatrix,A::PSparseMatrix,cache)\n\n\n\n\n\n","category":"method"},{"location":"reference/psparsematrix/#PartitionedArrays.repartition-Tuple{PSparseMatrix, PVector, Any, Any}","page":"PSparseMatrix","title":"PartitionedArrays.repartition","text":"repartition(A::PSparseMatrix,b::PVector,new_rows,new_cols;reuse=false)\n\n\n\n\n\n","category":"method"},{"location":"reference/psparsematrix/#PartitionedArrays.repartition!-Tuple{PSparseMatrix, PVector, PSparseMatrix, PVector, Any}","page":"PSparseMatrix","title":"PartitionedArrays.repartition!","text":"repartition!(B,c,A,b,cache)\n\nB::PSparseMatrix\nc::PVector\nA::PSparseMatrix\nb::PVector\ncache\n\n\n\n\n\n","category":"method"},{"location":"reference/primitives/#Parallel-primitives","page":"Parallel primitives","title":"Parallel primitives","text":"","category":"section"},{"location":"reference/primitives/#Gather","page":"Parallel primitives","title":"Gather","text":"","category":"section"},{"location":"reference/primitives/","page":"Parallel primitives","title":"Parallel primitives","text":"gather\ngather!\nallocate_gather","category":"page"},{"location":"reference/primitives/#PartitionedArrays.gather","page":"Parallel primitives","title":"PartitionedArrays.gather","text":"gather(snd;destination=MAIN)\n\nReturn an array whose first entry contains a copy of the elements of array snd collected in a vector. Another component different from the first one can be used to store the result by setting the optional key-word argument destination. Setting destination=:all, will store the result in all entries of rcv resulting in a \"gather all\" operation.\n\nExamples\n\njulia> using PartitionedArrays\n\njulia> snd = collect(1:3)\n3-element Vector{Int64}:\n 1\n 2\n 3\n\njulia> gather(snd,destination=3)\n3-element Vector{Vector{Int64}}:\n []\n []\n [1, 2, 3]\n\njulia> gather(snd,destination=:all)\n3-element Vector{Vector{Int64}}:\n [1, 2, 3]\n [1, 2, 3]\n [1, 2, 3]\n\n\n\n\n\n","category":"function"},{"location":"reference/primitives/#PartitionedArrays.gather!","page":"Parallel primitives","title":"PartitionedArrays.gather!","text":"gather!(rcv,snd;destination=MAIN)\n\nIn-place version of gather. It returns rcv. The result array rcv can be allocated with the helper function allocate_gather.\n\n\n\n\n\n","category":"function"},{"location":"reference/primitives/#PartitionedArrays.allocate_gather","page":"Parallel primitives","title":"PartitionedArrays.allocate_gather","text":"allocate_gather(snd;destination=MAIN)\n\nAllocate an array to be used in the first argument of gather!.\n\n\n\n\n\n","category":"function"},{"location":"reference/primitives/#Scatter","page":"Parallel primitives","title":"Scatter","text":"","category":"section"},{"location":"reference/primitives/","page":"Parallel primitives","title":"Parallel primitives","text":"scatter\nscatter!\nallocate_scatter","category":"page"},{"location":"reference/primitives/#PartitionedArrays.scatter","page":"Parallel primitives","title":"PartitionedArrays.scatter","text":"scatter(snd;source=MAIN)\n\nCopy the items in the collection snd[source] into an array of the same size and container type as snd. This function requires length(snd[source]) == length(snd).\n\nExamples\n\njulia> using PartitionedArrays\n\njulia> a = [Int[],[1,2,3],Int[]]\n3-element Vector{Vector{Int64}}:\n []\n [1, 2, 3]\n []\n\njulia> scatter(a,source=2)\n3-element Vector{Int64}:\n 1\n 2\n 3\n\n\n\n\n\n","category":"function"},{"location":"reference/primitives/#PartitionedArrays.scatter!","page":"Parallel primitives","title":"PartitionedArrays.scatter!","text":"scatter!(rcv,snd;source=1)\n\nIn-place version of scatter. The destination array rcv can be generated with the helper function allocate_scatter. It returns rcv.\n\n\n\n\n\n","category":"function"},{"location":"reference/primitives/#PartitionedArrays.allocate_scatter","page":"Parallel primitives","title":"PartitionedArrays.allocate_scatter","text":"allocate_scatter(snd;source=1)\n\nAllocate an array to be used in the first argument of scatter!.\n\n\n\n\n\n","category":"function"},{"location":"reference/primitives/#Multicast","page":"Parallel primitives","title":"Multicast","text":"","category":"section"},{"location":"reference/primitives/","page":"Parallel primitives","title":"Parallel primitives","text":"multicast\nmulticast!\nallocate_multicast","category":"page"},{"location":"reference/primitives/#PartitionedArrays.multicast","page":"Parallel primitives","title":"PartitionedArrays.multicast","text":"multicast(snd;source=MAIN)\n\nCopy snd[source] into a new array of the same size and type as snd.\n\nExamples\n\njulia> using PartitionedArrays\n\njulia> a = [0,0,2,0]\n4-element Vector{Int64}:\n 0\n 0\n 2\n 0\n\njulia> multicast(a,source=3)\n4-element Vector{Int64}:\n 2\n 2\n 2\n 2\n\n\n\n\n\n","category":"function"},{"location":"reference/primitives/#PartitionedArrays.multicast!","page":"Parallel primitives","title":"PartitionedArrays.multicast!","text":"multicast!(rcv,snd;source=1)\n\nIn-place version of multicast. The destination array rcv can be generated with the helper function allocate_multicast. It returns rcv.\n\n\n\n\n\n","category":"function"},{"location":"reference/primitives/#PartitionedArrays.allocate_multicast","page":"Parallel primitives","title":"PartitionedArrays.allocate_multicast","text":"allocate_multicast(snd;source=1)\n\nAllocate an array to be used in the first argument of multicast!.\n\n\n\n\n\n","category":"function"},{"location":"reference/primitives/#Scan","page":"Parallel primitives","title":"Scan","text":"","category":"section"},{"location":"reference/primitives/","page":"Parallel primitives","title":"Parallel primitives","text":"scan\nscan!","category":"page"},{"location":"reference/primitives/#PartitionedArrays.scan","page":"Parallel primitives","title":"PartitionedArrays.scan","text":"scan(op,a;init,type)\n\nReturn the scan of the values in a for the operation op. Use type=:inclusive or type=:exclusive to use an inclusive or exclusive scan. init will be added to all items in the result. Additionally, for exclusive scans, the first item in the result will be set to init.\n\nExamples\n\njulia> using PartitionedArrays\n\njulia> a = [2,4,1,3]\n4-element Vector{Int64}:\n 2\n 4\n 1\n 3\n\njulia> scan(+,a,type=:inclusive,init=0)\n4-element Vector{Int64}:\n  2\n  6\n  7\n 10\n\njulia> scan(+,a,type=:exclusive,init=1)\n4-element Vector{Int64}:\n 1\n 3\n 7\n 8\n\n\n\n\n\n","category":"function"},{"location":"reference/primitives/#PartitionedArrays.scan!","page":"Parallel primitives","title":"PartitionedArrays.scan!","text":"scan!(op,b,a;init,type)\n\nIn-place version of scan on the result b.\n\n\n\n\n\n","category":"function"},{"location":"reference/primitives/#Reduction","page":"Parallel primitives","title":"Reduction","text":"","category":"section"},{"location":"reference/primitives/","page":"Parallel primitives","title":"Parallel primitives","text":"reduction\nreduction!","category":"page"},{"location":"reference/primitives/#PartitionedArrays.reduction","page":"Parallel primitives","title":"PartitionedArrays.reduction","text":"reduction(op, a; destination=MAIN [,init])\n\nReduce the values in array a according with operation op and the initial value init and store the result in a new array of the same size as a at index destination.\n\nExamples\n\njulia> using PartitionedArrays\n\njulia> a = [1,3,2,4]\n4-element Vector{Int64}:\n 1\n 3\n 2\n 4\n\njulia> reduction(+,a;init=0,destination=2)\n4-element Vector{Int64}:\n  0\n 10\n  0\n  0\n\n\n\n\n\n","category":"function"},{"location":"reference/primitives/#PartitionedArrays.reduction!","page":"Parallel primitives","title":"PartitionedArrays.reduction!","text":"reduction!(op, b, a;destination=MAIN [,init])\n\nIn-place version of reduction on the result b.\n\n\n\n\n\n","category":"function"},{"location":"reference/primitives/#Exchange","page":"Parallel primitives","title":"Exchange","text":"","category":"section"},{"location":"reference/primitives/","page":"Parallel primitives","title":"Parallel primitives","text":"ExchangeGraph\nExchangeGraph(snd,rcv)\nExchangeGraph(snd)\nexchange\nexchange!\nallocate_exchange","category":"page"},{"location":"reference/primitives/#PartitionedArrays.ExchangeGraph","page":"Parallel primitives","title":"PartitionedArrays.ExchangeGraph","text":"struct ExchangeGraph{A}\n\nType representing a directed graph to be used in exchanges, see function exchange and exchange!.\n\nProperties\n\nsnd::A\nrcv::A\n\nsnd[i] contains a list of the outgoing neighbors of node i. rcv[i] contains a list of the incomming neighbors of node i. A is a vector-like container type.\n\nSupertype hierarchy\n\nExchangeGraph <: Any\n\n\n\n\n\n","category":"type"},{"location":"reference/primitives/#PartitionedArrays.ExchangeGraph-Tuple{Any, Any}","page":"Parallel primitives","title":"PartitionedArrays.ExchangeGraph","text":"ExchangeGraph(snd,rcv)\n\nCreate an instance of ExchangeGraph from the underlying fields.\n\n\n\n\n\n","category":"method"},{"location":"reference/primitives/#PartitionedArrays.ExchangeGraph-Tuple{Any}","page":"Parallel primitives","title":"PartitionedArrays.ExchangeGraph","text":"ExchangeGraph(snd; symmetric=false [rcv, neighbors,find_rcv_ids])\n\nCreate an ExchangeGraph object only from the lists of outgoing  neighbors in snd. In case the list of incoming neighbors is known, it can be passed as key-word argument by setting rcv and the rest of key-word arguments are ignored. If symmetric==true, then the incoming neighbors are set to snd. Otherwise, either the optional neighbors or  find_rcv_ids are considered, in that order.  neighbors is also an ExchangeGraph that contains a super set of the outgoing and incoming neighbors associated with snd. It is used to find the incoming neighbors rcv efficiently. If neighbors are not provided, then find_rcv_ids  is used (either the user-provided or a default one). find_rcv_ids is a function that implements an algorithm to find the  rcv side of the exchange graph out of the snd side information.\n\n\n\n\n\n","category":"method"},{"location":"reference/primitives/#PartitionedArrays.exchange","page":"Parallel primitives","title":"PartitionedArrays.exchange","text":"exchange(snd,graph::ExchangeGraph) -> Task\n\nSend the data in snd according the directed graph graph. This function returns immediately and returns a task that produces the result, allowing for latency hiding. Use fetch to wait and get the result. The object snd and rcv=fetch(exchange(snd,graph)) are array of vectors. The  value snd[i][j] is sent to node graph.snd[i][j]. The value rcv[i][j] is the one received from  node graph.rcv[i][j].\n\nExamples\n\njulia> using PartitionedArrays\n\njulia> snd_ids = [[3,4],[1,3],[1,4],[2]]\n4-element Vector{Vector{Int64}}:\n [3, 4]\n [1, 3]\n [1, 4]\n [2]\n\njulia> graph = ExchangeGraph(snd_ids)\nExchangeGraph{Vector{Vector{Int64}}} with 4 nodes\n\n\njulia> snd = [[10,10],[20,20],[30,30],[40]]\n4-element Vector{Vector{Int64}}:\n [10, 10]\n [20, 20]\n [30, 30]\n [40]\n\njulia> t = exchange(snd,graph);\n\njulia> rcv = fetch(t)\n4-element Vector{Vector{Int64}}:\n [20, 30]\n [40]\n [10, 20]\n [10, 30]\n\n\n\n\n\n","category":"function"},{"location":"reference/primitives/#PartitionedArrays.exchange!","page":"Parallel primitives","title":"PartitionedArrays.exchange!","text":"exchange!(rcv,snd,graph::ExchangeGraph) -> Task\n\nIn-place and asynchronous version of exchange. This function returns immediately and returns a task that produces rcv with the updated values. Use fetch to get the updated version of rcv. The input rcv can be allocated with allocate_exchange.\n\n\n\n\n\n","category":"function"},{"location":"reference/primitives/#PartitionedArrays.allocate_exchange","page":"Parallel primitives","title":"PartitionedArrays.allocate_exchange","text":"allocate_exchange(snd,graph::ExchangeGraph)\n\nAllocate the result to be used in the first argument of exchange!.\n\n\n\n\n\n","category":"function"},{"location":"reference/pvector/#PVector","page":"PVector","title":"PVector","text":"","category":"section"},{"location":"reference/pvector/#Type-signature","page":"PVector","title":"Type signature","text":"","category":"section"},{"location":"reference/pvector/","page":"PVector","title":"PVector","text":"PVector","category":"page"},{"location":"reference/pvector/#PartitionedArrays.PVector","page":"PVector","title":"PartitionedArrays.PVector","text":"struct PVector{V,A,B,...}\n\nPVector (partitioned vector) is a type representing a vector whose entries are distributed (a.k.a. partitioned) over different parts for distributed-memory parallel computations.\n\nThis type overloads numerous array-like operations with corresponding parallel implementations.\n\nProperties\n\nvector_partition::A\nindex_partition::B\n\nvector_partition[i] contains the vector of local values of the i-th part in the data distribution. The first type parameter V corresponds to typeof(values[i]) i.e. the vector type used to store the local values. The item index_partition[i] implements the AbstractLocalIndices interface providing information about the local, own, and ghost indices in the i-th part.\n\nThe rest of fields of this struct and type parameters are private.\n\nSupertype hierarchy\n\nPVector{V,A,B,...} <: AbstractVector{T}\n\nwith T=eltype(V).\n\n\n\n\n\n","category":"type"},{"location":"reference/pvector/#Accessors","page":"PVector","title":"Accessors","text":"","category":"section"},{"location":"reference/pvector/","page":"PVector","title":"PVector","text":"local_values(::PVector)\nown_values(::PVector)\nghost_values(::PVector)","category":"page"},{"location":"reference/pvector/#PartitionedArrays.local_values-Tuple{PVector}","page":"PVector","title":"PartitionedArrays.local_values","text":"local_values(a::PVector)\n\nGet a vector of vectors containing the local values in each part of a.\n\nThe indices of the returned vectors can be mapped to global indices, own indices, ghost indices, and owner by using local_to_global, local_to_own, local_to_ghost, and local_to_owner, respectively.\n\n\n\n\n\n","category":"method"},{"location":"reference/pvector/#PartitionedArrays.own_values-Tuple{PVector}","page":"PVector","title":"PartitionedArrays.own_values","text":"own_values(a::PVector)\n\nGet a vector of vectors containing the own values in each part of a.\n\nThe indices of the returned vectors can be mapped to global indices, local indices, and owner by using own_to_global, own_to_local, and own_to_owner, respectively.\n\n\n\n\n\n","category":"method"},{"location":"reference/pvector/#PartitionedArrays.ghost_values-Tuple{PVector}","page":"PVector","title":"PartitionedArrays.ghost_values","text":"ghost_values(a::PVector)\n\nGet a vector of vectors containing the ghost values in each part of a.\n\nThe indices of the returned matrices can be mapped to global indices, local indices, and owner by using ghost_to_global, ghost_to_local, and ghost_to_owner, respectively.\n\n\n\n\n\n","category":"method"},{"location":"reference/pvector/#Constructors","page":"PVector","title":"Constructors","text":"","category":"section"},{"location":"reference/pvector/","page":"PVector","title":"PVector","text":"PVector(a,b)\nPVector{V}(::UndefInitializer,b) where V\npvector(f,a)\npvector(f,a,b,c)\npvector!\npfill\npzeros\npones\nprand\nprandn","category":"page"},{"location":"reference/pvector/#PartitionedArrays.PVector-Tuple{Any, Any}","page":"PVector","title":"PartitionedArrays.PVector","text":"PVector(vector_partition,index_partition)\n\nCreate an instance of PVector from the underlying properties vector_partition and index_partition.\n\n\n\n\n\n","category":"method"},{"location":"reference/pvector/#PartitionedArrays.PVector-Union{Tuple{V}, Tuple{UndefInitializer, Any}} where V","page":"PVector","title":"PartitionedArrays.PVector","text":"PVector{V}(undef,index_partition)\nPVector(undef,index_partition)\n\nCreate an instance of PVector with local uninitialized values stored in a vector of type V (which defaults to V=Vector{Float64}).\n\n\n\n\n\n","category":"method"},{"location":"reference/pvector/#PartitionedArrays.pvector-Tuple{Any, Any}","page":"PVector","title":"PartitionedArrays.pvector","text":"pvector(f,index_partition)\n\nEquivalent to \n\nvector_partition = map(f,index_partition)\nPVector(vector_partition,index_partition)\n\n\n\n\n\n","category":"method"},{"location":"reference/pvector/#PartitionedArrays.pvector-NTuple{4, Any}","page":"PVector","title":"PartitionedArrays.pvector","text":"pvector([f,]I,V,index_partition;kwargs...) -> Task\n\nCrate an instance of PVector by setting arbitrary entries from each of the underlying parts. It returns a task that produces the instance of PVector allowing latency hiding while performing the communications needed in its setup.\n\n\n\n\n\n","category":"method"},{"location":"reference/pvector/#PartitionedArrays.pvector!","page":"PVector","title":"PartitionedArrays.pvector!","text":"pvector!(B::PVector,V,cache)\n\n\n\n\n\n","category":"function"},{"location":"reference/pvector/#PartitionedArrays.pfill","page":"PVector","title":"PartitionedArrays.pfill","text":"pfill(v,index_partition)\n\n\n\n\n\n","category":"function"},{"location":"reference/pvector/#PartitionedArrays.pzeros","page":"PVector","title":"PartitionedArrays.pzeros","text":"pzeros([T,]index_partition)\n\nEquivalent to\n\npfill(zero(T),index_partition)\n\n\n\n\n\n","category":"function"},{"location":"reference/pvector/#PartitionedArrays.pones","page":"PVector","title":"PartitionedArrays.pones","text":"pones([T,]index_partition)\n\nEquivalent to\n\npfill(one(T),index_partition)\n\n\n\n\n\n","category":"function"},{"location":"reference/pvector/#PartitionedArrays.prand","page":"PVector","title":"PartitionedArrays.prand","text":"prand([rng,][s,]index_partition)\n\nCreate a PVector object with uniform random values and the data partition in index_partition. The optional arguments have the same meaning and default values as in rand.\n\n\n\n\n\n","category":"function"},{"location":"reference/pvector/#PartitionedArrays.prandn","page":"PVector","title":"PartitionedArrays.prandn","text":"prandn([rng,][s,]index_partition)\n\nCreate a PVector object with normally distributed random values and the data partition in index_partition. The optional arguments have the same meaning and default values as in randn.\n\n\n\n\n\n","category":"function"},{"location":"reference/pvector/#Assembly","page":"PVector","title":"Assembly","text":"","category":"section"},{"location":"reference/pvector/","page":"PVector","title":"PVector","text":"assemble!(::PVector)\nassemble(::PVector,rows)\nassemble!(::PVector,::PVector,cache)\nconsistent!(::PVector)\nconsistent(::PVector,cols)\nconsistent!(::PVector,::PVector,cache)","category":"page"},{"location":"reference/pvector/#PartitionedArrays.assemble!-Tuple{PVector}","page":"PVector","title":"PartitionedArrays.assemble!","text":"assemble!([op,] a::PVector) -> Task\n\nTransfer the ghost values to their owner part and insert them according with the insertion operation op (+ by default). It returns a task that produces a with updated values. After the transfer, the source ghost values are set to zero.\n\nExamples\n\njulia> using PartitionedArrays\n\njulia> rank = LinearIndices((2,));\n\njulia> row_partition = uniform_partition(rank,6,true);\n\njulia> map(local_to_global,row_partition)\n2-element Vector{PartitionedArrays.BlockPartitionLocalToGlobal{1, Vector{Int32}}}:\n [1, 2, 3, 4]\n [3, 4, 5, 6]\n\njulia> a = pones(row_partition)\n6-element PVector partitioned into 2 parts of type Vector{Float64}\n\njulia> local_values(a)\n2-element Vector{Vector{Float64}}:\n [1.0, 1.0, 1.0, 1.0]\n [1.0, 1.0, 1.0, 1.0]\n\njulia> assemble!(a) |> wait\n\njulia> local_values(a)\n2-element Vector{Vector{Float64}}:\n [1.0, 1.0, 2.0, 0.0]\n [0.0, 2.0, 1.0, 1.0]\n\n\n\n\n\n","category":"method"},{"location":"reference/pvector/#PartitionedArrays.assemble-Tuple{PVector, Any}","page":"PVector","title":"PartitionedArrays.assemble","text":"assemble(v::PVector[,rows];reuse=false)\n\n\n\n\n\n","category":"method"},{"location":"reference/pvector/#PartitionedArrays.assemble!-Tuple{PVector, PVector, Any}","page":"PVector","title":"PartitionedArrays.assemble!","text":"assemble!(w::PVector,v::PVector,cache)\n\n\n\n\n\n","category":"method"},{"location":"reference/pvector/#PartitionedArrays.consistent!-Tuple{PVector}","page":"PVector","title":"PartitionedArrays.consistent!","text":"consistent!(a::PVector) -> Task\n\nMake the local values of a globally consistent. I.e., the ghost values are updated with the corresponding own value in the part that owns the associated global global id.\n\nExamples\n\njulia> using PartitionedArrays\n\njulia> rank = LinearIndices((2,));\n\njulia> row_partition = uniform_partition(rank,6,true);\n\njulia> map(local_to_global,row_partition)\n2-element Vector{PartitionedArrays.BlockPartitionLocalToGlobal{1, Vector{Int32}}}:\n [1, 2, 3, 4]\n [3, 4, 5, 6]\n\njulia> a = pvector(inds->fill(part_id(inds),length(inds)),row_partition)\n6-element PVector partitioned into 2 parts of type Vector{Int32}\n\njulia> local_values(a)\n2-element Vector{Vector{Int32}}:\n [1, 1, 1, 1]\n [2, 2, 2, 2]\n\njulia> consistent!(a) |> wait\n\njulia> local_values(a)\n2-element Vector{Vector{Int32}}:\n [1, 1, 1, 2]\n [1, 2, 2, 2]\n\n\n\n\n\n","category":"method"},{"location":"reference/pvector/#PartitionedArrays.consistent-Tuple{PVector, Any}","page":"PVector","title":"PartitionedArrays.consistent","text":"consistent(v::PVector,rows;reuse=false)\n\n\n\n\n\n","category":"method"},{"location":"reference/pvector/#PartitionedArrays.consistent!-Tuple{PVector, PVector, Any}","page":"PVector","title":"PartitionedArrays.consistent!","text":"consistent!(w::PVector,v::PVector,cache)\n\n\n\n\n\n","category":"method"},{"location":"reference/pvector/#Re-partition","page":"PVector","title":"Re-partition","text":"","category":"section"},{"location":"reference/pvector/","page":"PVector","title":"PVector","text":"repartition(::PVector,rows)\nrepartition!(::PVector,::PVector,cache)","category":"page"},{"location":"reference/pvector/#PartitionedArrays.repartition-Tuple{PVector, Any}","page":"PVector","title":"PartitionedArrays.repartition","text":"repartition(v::PVector,new_partition;reuse=false)\n\n\n\n\n\n","category":"method"},{"location":"reference/pvector/#PartitionedArrays.repartition!-Tuple{PVector, PVector, Any}","page":"PVector","title":"PartitionedArrays.repartition!","text":"repartition!(w::PVector,v::PVector[,cache];reversed=false)\n\n\n\n\n\n","category":"method"},{"location":"reference/advanced/#Advanced","page":"Advanced","title":"Advanced","text":"","category":"section"},{"location":"reference/advanced/#Custom-partitions","page":"Advanced","title":"Custom partitions","text":"","category":"section"},{"location":"reference/advanced/","page":"Advanced","title":"Advanced","text":"LocalIndices\nLocalIndices(a,b,c,d)\nOwnAndGhostIndices\nOwnAndGhostIndices(own::OwnIndices,ghost::GhostIndices)\nOwnIndices\nOwnIndices(a,b,c)\nGhostIndices\nGhostIndices(a,b,c)","category":"page"},{"location":"reference/advanced/#PartitionedArrays.LocalIndices","page":"Advanced","title":"PartitionedArrays.LocalIndices","text":"struct LocalIndices\n\nContainer for arbitrary local indices.\n\nProperties\n\nn_global::Int: Number of global indices.\nowner::Int32: Id of the part that stores the local indices\nlocal_to_global::Vector{Int}:  Global ids of the local indices in this part.  local_to_global[i_local] is the global id corresponding to the local index number i_local.\nlocal_to_owner::Vector{Int32}: Owners of the local ids. local_to_owner[i_local]is the id of the owner of the local index number i_local.\n\nSupertype hierarchy\n\nLocalIndices <: AbstractLocalIndices\n\n\n\n\n\n","category":"type"},{"location":"reference/advanced/#PartitionedArrays.LocalIndices-NTuple{4, Any}","page":"Advanced","title":"PartitionedArrays.LocalIndices","text":"LocalIndices(n_global,owner,local_to_global,local_to_owner)\n\nBuild an instance of LocalIndices from the underlying properties n_global, owner, local_to_global, and local_to_owner.  The types of these variables need to match the type of the properties in LocalIndices.\n\n\n\n\n\n","category":"method"},{"location":"reference/advanced/#PartitionedArrays.OwnAndGhostIndices","page":"Advanced","title":"PartitionedArrays.OwnAndGhostIndices","text":"OwnAndGhostIndices\n\nContainer for local indices stored as own and ghost indices separately. Local indices are defined by concatenating own and ghost ones.\n\nProperties\n\nown::OwnIndices: Container for the own indices.\nghost::GhostIndices: Container for the ghost indices.\nglobal_to_owner: [optional: it can be nothing] Vector containing the owner of each global id.\n\nSupertype hierarchy\n\nOwnAndGhostIndices{A} <: AbstractLocalIndices\n\nwhere A=typeof(global_to_owner).\n\n\n\n\n\n","category":"type"},{"location":"reference/advanced/#PartitionedArrays.OwnAndGhostIndices-Tuple{OwnIndices, GhostIndices}","page":"Advanced","title":"PartitionedArrays.OwnAndGhostIndices","text":"OwnAndGhostIndices(own::OwnIndices,ghost::GhostIndices,global_to_owner=nothing)\n\nBuild an instance of OwnAndGhostIndices from the underlying properties own, ghost, and global_to_owner.\n\n\n\n\n\n","category":"method"},{"location":"reference/advanced/#PartitionedArrays.OwnIndices","page":"Advanced","title":"PartitionedArrays.OwnIndices","text":"struct OwnIndices\n\nContainer for own indices.\n\nProperties\n\nn_global::Int: Number of global indices\nowner::Int32: Id of the part that owns these indices\nown_to_global::Vector{Int}: Global ids of the indices owned by this part. own_to_global[i_own] is the global id corresponding to the own index number i_own. \n\nSupertype hierarchy\n\nOwnIndices <: Any\n\n\n\n\n\n","category":"type"},{"location":"reference/advanced/#PartitionedArrays.OwnIndices-Tuple{Any, Any, Any}","page":"Advanced","title":"PartitionedArrays.OwnIndices","text":"OwnIndices(n_global,owner,own_to_global)\n\nBuild an instance of OwnIndices from the underlying properties n_global, owner, and own_to_global. The types of these variables need to match the type of the properties in OwnIndices.\n\n\n\n\n\n","category":"method"},{"location":"reference/advanced/#PartitionedArrays.GhostIndices","page":"Advanced","title":"PartitionedArrays.GhostIndices","text":"struct GhostIndices\n\nContainer for ghost indices.\n\nProperties\n\nn_global::Int: Number of global indices\nghost_to_global::Vector{Int}: Global ids of the ghost indices in this part. ghost_to_global[i_ghost] is the global id corresponding to the ghost index number i_ghost. \nghost_to_owner::Vector{Int32}: Owners of the ghost ids. ghost_to_owner[i_ghost]is the id of the owner of the ghost index number i_ghost.\n\nSupertype hierarchy\n\nGhostIndices <: Any\n\n\n\n\n\n","category":"type"},{"location":"reference/advanced/#PartitionedArrays.GhostIndices-Tuple{Any, Any, Any}","page":"Advanced","title":"PartitionedArrays.GhostIndices","text":"GhostIndices(n_global,ghost_to_global,ghost_to_owner)\n\nBuild an instance of GhostIndices from the underlying fields n_global, ghost_to_global, and ghost_to_owner. The types of these variables need to match the type of the properties in GhostIndices.\n\n\n\n\n\n","category":"method"},{"location":"reference/advanced/#Transform-partitions","page":"Advanced","title":"Transform partitions","text":"","category":"section"},{"location":"reference/advanced/","page":"Advanced","title":"Advanced","text":"replace_ghost\nunion_ghost\npermute_indices\nfind_owner","category":"page"},{"location":"reference/advanced/#PartitionedArrays.replace_ghost","page":"Advanced","title":"PartitionedArrays.replace_ghost","text":"replace_ghost(indices,gids,owners)\n\nReplaces the ghost indices in indices with global ids in gids and owners in   owners. Returned object takes ownership of gids  and owners. This method  only makes sense if indices stores ghost ids in separate vectors like in OwnAndGhostIndices. gids should be unique and not being owned by  indices.\n\n\n\n\n\n","category":"function"},{"location":"reference/advanced/#PartitionedArrays.union_ghost","page":"Advanced","title":"PartitionedArrays.union_ghost","text":"union_ghost(indices,gids,owners)\n\nMake the union of the ghost indices in indices with   the global indices gids and owners owners.  Return an object  of the same type as indices with the new ghost indices and the same  own indices as in indices.  The result does not take ownership of gids  and owners. \n\n\n\n\n\n","category":"function"},{"location":"reference/advanced/#PartitionedArrays.permute_indices","page":"Advanced","title":"PartitionedArrays.permute_indices","text":"permute_indices(indices,perm)\n\n\n\n\n\n","category":"function"},{"location":"reference/advanced/#PartitionedArrays.find_owner","page":"Advanced","title":"PartitionedArrays.find_owner","text":"find_owner(index_partition,global_ids)\n\nFind the owners of the global ids in global_ids. The input global_ids is a vector of vectors distributed over the same parts as index_partition. Each part will look for the owners in parallel, when using a parallel back-end.\n\nExample\n\njulia> using PartitionedArrays\n\njulia> rank = LinearIndices((4,));\n\njulia> index_partition = uniform_partition(rank,10)\n4-element Vector{PartitionedArrays.LocalIndicesWithConstantBlockSize{1}}:\n [1, 2]\n [3, 4]\n [5, 6, 7]\n [8, 9, 10]\n\njulia> gids = [[3],[4,5],[7,2],[9,10,1]]\n4-element Vector{Vector{Int64}}:\n [3]\n [4, 5]\n [7, 2]\n [9, 10, 1]\n\njulia> find_owner(index_partition,gids)\n4-element Vector{Vector{Int32}}:\n [2]\n [2, 3]\n [3, 1]\n [4, 4, 1]\n\n\n\n\n\n","category":"function"},{"location":"reference/advanced/#Transform-indices","page":"Advanced","title":"Transform indices","text":"","category":"section"},{"location":"reference/advanced/","page":"Advanced","title":"Advanced","text":"to_global!\nto_local!","category":"page"},{"location":"reference/advanced/#PartitionedArrays.to_global!","page":"Advanced","title":"PartitionedArrays.to_global!","text":"to_global!(I,indices)\n\nTransform the local indices in I into global ids according to indices.\n\n\n\n\n\n","category":"function"},{"location":"reference/advanced/#PartitionedArrays.to_local!","page":"Advanced","title":"PartitionedArrays.to_local!","text":"to_local!(I,indices)\n\nTransform the global indices in I into local ids according to indices.\n\n\n\n\n\n","category":"function"},{"location":"reference/advanced/#Local-vector-storage","page":"Advanced","title":"Local vector storage","text":"","category":"section"},{"location":"reference/advanced/","page":"Advanced","title":"Advanced","text":"OwnAndGhostVectors\nOwnAndGhostVectors(a,b,c)","category":"page"},{"location":"reference/advanced/#PartitionedArrays.OwnAndGhostVectors","page":"Advanced","title":"PartitionedArrays.OwnAndGhostVectors","text":"struct OwnAndGhostVectors{A,C,T}\n\nVector type that stores the local values of a PVector instance using a vector of own values, a vector of ghost values, and a permutation. This is not the default data layout of PVector (which is a plain vector), but corresponds to the layout of distributed vectors in other packages, such as PETSc. Use this type to avoid duplicating memory when passing data to these other packages.\n\nProperties\n\nown_values::A: The vector of own values.\nghost_values::A: The vector of ghost values.\npermumation::C: A permutation vector such that vcat(own_values,ghost_values)[permutation] corresponds to the local values.\n\nSupertype hierarchy\n\nOwnAndGhostVectors{A,C,T} <: AbstractVector{T}\n\n\n\n\n\n","category":"type"},{"location":"reference/advanced/#PartitionedArrays.OwnAndGhostVectors-Tuple{Any, Any, Any}","page":"Advanced","title":"PartitionedArrays.OwnAndGhostVectors","text":"OwnAndGhostVectors(own_values,ghost_values,permutation)\n\nBuild an instance of OwnAndGhostVectors from the underlying fields.\n\n\n\n\n\n","category":"method"},{"location":"reference/advanced/#Assembly","page":"Advanced","title":"Assembly","text":"","category":"section"},{"location":"reference/advanced/","page":"Advanced","title":"Advanced","text":"assembly_graph\nassembly_neighbors\nassembly_local_indices","category":"page"},{"location":"reference/advanced/#PartitionedArrays.assembly_graph","page":"Advanced","title":"PartitionedArrays.assembly_graph","text":"assembly_graph(index_partition;kwargs...)\n\nReturn an instance of ExchangeGraph representing the communication graph needed to perform assembly of distributed vectors defined on the index partition index_partition. kwargs are delegated to ExchangeGraph in order to find the receiving neighbors from the sending ones.\n\nEquivalent to\n\nneighbors = assembly_neighbors(index_partition;kwargs...)\nExchangeGraph(neighbors...)\n\n\n\n\n\n","category":"function"},{"location":"reference/advanced/#PartitionedArrays.assembly_neighbors","page":"Advanced","title":"PartitionedArrays.assembly_neighbors","text":"neigs_snd, neigs_rcv = assembly_neighbors(index_partition;kwargs...)\n\nReturn the ids of the neighbor parts from we send and receive data respectively in the assembly of distributed vectors defined on the index partition index_partition. partition index_partition. kwargs are delegated to ExchangeGraph in order to find the receiving neighbors from the sending ones.\n\n\n\n\n\n","category":"function"},{"location":"reference/advanced/#PartitionedArrays.assembly_local_indices","page":"Advanced","title":"PartitionedArrays.assembly_local_indices","text":"ids_snd, ids_rcv = assembly_local_indices(index_partition)\n\nReturn the local ids to be sent and received   in the assembly of distributed vectors defined on the index partition index_partition.\n\nLocal values corresponding to the local indices in ids_snd[i] (respectively ids_rcv[i]) are sent to part neigs_snd[i] (respectively neigs_rcv[i]), where neigs_snd, neigs_rcv = assembly_neighbors(index_partition).\n\n\n\n\n\n","category":"function"},{"location":"reference/backends/#Back-ends","page":"Back-ends","title":"Back-ends","text":"","category":"section"},{"location":"reference/backends/#MPI","page":"Back-ends","title":"MPI","text":"","category":"section"},{"location":"reference/backends/","page":"Back-ends","title":"Back-ends","text":"Modules = [PartitionedArrays]\nPages = [\"mpi_array.jl\"]","category":"page"},{"location":"reference/backends/#PartitionedArrays.MPIArray","page":"Back-ends","title":"PartitionedArrays.MPIArray","text":"MPIArray{T,N}\n\nRepresent an array of element type T and number of dimensions N, where each item in the array is stored in a separate MPI process. I.e., each MPI rank stores only one item. For arrays that can store more than one item per rank see PVector or PSparseMatrix. This struct implements the Julia array interface. However, using setindex! and getindex! is disabled for performance reasons (communication cost).\n\nProperties\n\nThe fields of this struct (and the inner constructors) are private. To generate an instance of MPIArray use function distribute_with_mpi.\n\nSupertype hierarchy\n\nMPIArray{T,N} <: AbstractArray{T,N}\n\n\n\n\n\n","category":"type"},{"location":"reference/backends/#PartitionedArrays.distribute_with_mpi-Tuple{Any}","page":"Back-ends","title":"PartitionedArrays.distribute_with_mpi","text":"distribute_with_mpi(a;comm::MPI.Comm=MPI.COMM_WORLD,duplicate_comm=true)\n\nCreate an MPIArray instance by distributing the items in the collection a over the ranks of the given MPI communicator comm. Each rank receives exactly one item, thus length(a)  and the communicator size need to match. For arrays that can store more than one item per rank see PVector or PSparseMatrix. If duplicate_comm=false the result will take ownership of the given communicator. Otherwise, a copy will be done with MPI.Comm_dup(comm).\n\nnote: Note\nThis function calls MPI.Init() if MPI is not initialized yet.\n\n\n\n\n\n","category":"method"},{"location":"reference/backends/#PartitionedArrays.find_rcv_ids_ibarrier-Union{Tuple{MPIArray{<:AbstractVector{T}}}, Tuple{T}} where T","page":"Back-ends","title":"PartitionedArrays.find_rcv_ids_ibarrier","text":"Implements Alg. 2 in https://dl.acm.org/doi/10.1145/1837853.1693476  The algorithm's complexity is claimed to be O(log(p))\n\n\n\n\n\n","category":"method"},{"location":"reference/backends/#PartitionedArrays.with_mpi-Tuple{Any}","page":"Back-ends","title":"PartitionedArrays.with_mpi","text":"with_mpi(f;comm=MPI.COMM_WORLD,duplicate_comm=true)\n\nCall f(a->distribute_with_mpi(a;comm,duplicate_comm)) and abort MPI if there was an error.  This is the safest way of running the function f using MPI.\n\nnote: Note\nThis function calls MPI.Init() if MPI is not initialized yet.\n\n\n\n\n\n","category":"method"},{"location":"reference/backends/#Debug","page":"Back-ends","title":"Debug","text":"","category":"section"},{"location":"reference/backends/","page":"Back-ends","title":"Back-ends","text":"Modules = [PartitionedArrays]\nPages = [\"debug_array.jl\"]","category":"page"},{"location":"reference/backends/#PartitionedArrays.DebugArray","page":"Back-ends","title":"PartitionedArrays.DebugArray","text":"struct DebugArray{T,N}\n\nData structure that emulates the limitations of MPIArray, but that can be used on a standard sequential (a.k.a. serial) Julia session. This struct implements the Julia array interface. Like for MPIArray, using setindex! and getindex on DebugArray is disabled since this will not be efficient in actual parallel runs (communication cost).\n\nProperties\n\nThe fields of this struct are private.\n\nSupertype hierarchy\n\nDebugArray{T,N} <: AbstractArray{T,N}\n\n\n\n\n\n","category":"type"},{"location":"reference/backends/#PartitionedArrays.DebugArray-Tuple{Any}","page":"Back-ends","title":"PartitionedArrays.DebugArray","text":"DebugArray(a)\n\nCreate a DebugArray{T,N} data object from the items in collection a, where T=eltype(a) and N=ndims(a) . If a::Array{T,N}, then the result takes ownership of the input. Otherwise, a copy of the input is created.\n\n\n\n\n\n","category":"method"},{"location":"reference/backends/#PartitionedArrays.with_debug-Tuple{Any}","page":"Back-ends","title":"PartitionedArrays.with_debug","text":"with_debug(f)\n\nCall f(DebugArray).\n\n\n\n\n\n","category":"method"},{"location":"refindex/#Index","page":"Index","title":"Index","text":"","category":"section"},{"location":"refindex/","page":"Index","title":"Index","text":"","category":"page"},{"location":"usage/#Usage","page":"Usage","title":"Usage","text":"","category":"section"},{"location":"usage/","page":"Usage","title":"Usage","text":"PartitionedArrays considers a data-oriented programming model that allows one to write distributed algorithms in a generic way, independent from the message passing back-end used to run them. The basic abstraction of this model consists in expressing distributed data using array containers. The particular container type will depend on the back-end used to run the code in parallel. MPI is one of the possible back-ends, used to run large cases on computational clusters. However, one can also use serial arrays to prototype and debug complex codes in an effective way.","category":"page"},{"location":"usage/#Basic-usage","page":"Usage","title":"Basic usage","text":"","category":"section"},{"location":"usage/","page":"Usage","title":"Usage","text":"We want each rank in a distributed system to print its rank id and the total number of ranks. The distributed data are the rank ids. If we have an array with all rank ids, printing the messages is trivial with map function.","category":"page"},{"location":"usage/","page":"Usage","title":"Usage","text":"np = 4\nranks = LinearIndices((np,))\nmap(ranks) do rank\n   println(\"I am proc $rank of $np.\")\nend","category":"page"},{"location":"usage/","page":"Usage","title":"Usage","text":"Previous code is not parallel (yet). However, it can be easily parallelized if one considers a suitable distributed array type that overloads map with a parallel implementation.","category":"page"},{"location":"usage/","page":"Usage","title":"Usage","text":"# hello_mpi.jl\nusing PartitionedArrays\nnp = 4\nranks = distribute_with_mpi(LinearIndices((np,)))\nmap(ranks) do rank\n   println(\"I am proc $rank of $np.\")\nend","category":"page"},{"location":"usage/","page":"Usage","title":"Usage","text":"Now this code is parallel. Function distribute_with_mpi takes an array and distributes it over the different ranks of a given MPI communicator (a duplicate of MPI.COMM_WORLD by default). The type of the result is an array type called MPIArray, which overloads function map with a parallel implementation. Function distribute_with_mpi assigns exactly one item in the input array to each rank in the communicator. Thus, the resulting array ranks will be distributed in such a way that each MPI rank will get an integer corresponding to its (1-based) rank id. If we place   this code in a file called \"hello_mpi.jl\", we can run it as any Julia applications using the MPI API in   MPI.jl. For instance,","category":"page"},{"location":"usage/","page":"Usage","title":"Usage","text":"using MPI\nmpiexec(cmd->run(`$cmd -np 4 julia --project=. hello_mpi.jl`))","category":"page"},{"location":"usage/","page":"Usage","title":"Usage","text":"The construction of the array ranks containing the rank ids is just the first step of a computation using PartitionedArrays. See the Examples for more interesting cases.","category":"page"},{"location":"usage/#Debugging","page":"Usage","title":"Debugging","text":"","category":"section"},{"location":"usage/","page":"Usage","title":"Usage","text":"One of the main advantages of PartitionedArrays is that it allows one to write and debug your parallel code without using MPI. This makes possible to use the standard Julia development workflow (e.g., Revise)  when implementing distributed applications, which is certainly useful. This ability comes from the fact that one can use standard serial Julia arrays to test your application based on PartitionedArrays. However, the array type MPIArray resulting after distributing data over MPI processes, is not as flexible as the standard arrays in Julia. There are operations that are not allowed for MPIArray, mainly for performance reasons. One of them is indexing the array at arbitrary indices. In consequence, code that runs with the common Julia arrays might fall when switching to MPI. In order to anticipate these type of errors, PartitionedArrays provides an special array type called DebugArray for debugging purposes. The type DebugArray tries to mimic the limitations of MPIArray but it is just a wrapper to a standard Julia array and therefore can be used in a standard Julia session.","category":"page"},{"location":"usage/","page":"Usage","title":"Usage","text":"using PartitionedArrays\nnp = 4\nranks = DebugArray(LinearIndices((np,)))\nranks[3] # Error!","category":"page"},{"location":"usage/","page":"Usage","title":"Usage","text":"The last line of previous code will throw an error telling that scalar indexing is not allowed. This is to mimic the error you would get in production when using MPI.","category":"page"},{"location":"usage/","page":"Usage","title":"Usage","text":"using PartitionedArrays\nwith_mpi() do distribute\n    np = 4\n    ranks = distribute(LinearIndices((np,)))\n    ranks[3] # Error!\nend","category":"page"},{"location":"usage/","page":"Usage","title":"Usage","text":"We also provide function with_debug which allows to easily switch from one back-end to the other. For instance, if we define the following main function","category":"page"},{"location":"usage/","page":"Usage","title":"Usage","text":"using PartitionedArrays\nfunction main(distribute)\n    np = 4\n    ranks = distribute(LinearIndices((np,)))\n    map(ranks) do rank\n       println(\"I am proc $rank of $np\")\n    end\nend","category":"page"},{"location":"usage/","page":"Usage","title":"Usage","text":"then with_debug(main) and with_mpi(main) will run the code using the debug back-end and MPI respectively. If you want to run in using native Julia arrays, you can simply call main(identity). Make sure that your code works using DebugArray before moving to MPI.","category":"page"},{"location":"usage/#Running-MPI-code-safely","page":"Usage","title":"Running MPI code safely","text":"","category":"section"},{"location":"usage/","page":"Usage","title":"Usage","text":"MPI applications should call MPI.Abort if they stop prematurely (e.g., by an error). The Julia error handling system is not aware of that. For this reasons, codes like the following one will crash and stop without calling MPI.Abort.","category":"page"},{"location":"usage/","page":"Usage","title":"Usage","text":"using PartitionedArrays\nnp = 3\nranks = distribute_with_mpi(LinearIndices((np,)))\nmap(ranks) do rank\n    if rank == 2\n        error(\"I have crashed\")\n    end\nend","category":"page"},{"location":"usage/","page":"Usage","title":"Usage","text":"Even worse, the code will crash only in the 2nd MPI process. The other processes will finish normally. This can lead to zombie MPI processes running in the background (and provably consuming quota in your cluster account until the queuing system kills them). To fix this, PartitionedArrays provides function with_mpi. We rewrite the previous example using it.","category":"page"},{"location":"usage/","page":"Usage","title":"Usage","text":"using PartitionedArrays\nwith_mpi() do distribute\n    np = 3\n    ranks = distribute(LinearIndices((np,)))\n    map(ranks) do rank\n        if rank == 2\n            error(\"I have crashed\")\n        end\n    end\nend","category":"page"},{"location":"usage/","page":"Usage","title":"Usage","text":"Essentially, with_mpi(f) calls f(distribute_with_mpi) in a try-catch block. If some error is cached,  MPI.Abort will be called, safely finishing all the MPI processes, also the ones that did not experienced the error.","category":"page"},{"location":"usage/#Benchmarking-distributed-codes","page":"Usage","title":"Benchmarking distributed codes","text":"","category":"section"},{"location":"usage/","page":"Usage","title":"Usage","text":"When using MPI, the computational time to run some code can be different for each one of the processes. Usually, one measures the time for each process and computes some statistics of the resulting values. To this end, the library provides a special timer type called PTimer. In the following example we force different computation times at each of the processes by sleeping a value proportional to the rank id. When displayed, the instance of PTimer shows some statistics of the times over the different processes.","category":"page"},{"location":"usage/","page":"Usage","title":"Usage","text":"using PartitionedArrays\nwith_mpi() do distribute\n    np = 3\n    ranks = distribute(LinearIndices((np,)))\n    t = PTimer(ranks)\n    tic!(t)\n    map(ranks) do rank\n        sleep(rank)\n    end\n    toc!(t,\"Sleep\")\n    display(t)\nend","category":"page"},{"location":"usage/","page":"Usage","title":"Usage","text":"\nSection         max         min         avg\n\nSleep     3.021e+00   1.021e+00   2.021e+00\n","category":"page"},{"location":"usage/","page":"Usage","title":"Usage","text":"This mechanism also works for the other back-ends. For sequential ones, it provides the type spend by all parts combined.","category":"page"},{"location":"usage/","page":"Usage","title":"Usage","text":"using PartitionedArrays\nwith_debug() do distribute\n    np = 3\n    ranks = distribute(LinearIndices((np,)))\n    t = PTimer(ranks)\n    tic!(t)\n    map(ranks) do rank\n        sleep(rank)\n    end\n    toc!(t,\"Sleep\")\n    display(t)\nend","category":"page"},{"location":"usage/","page":"Usage","title":"Usage","text":"\nSection         max         min         avg\n\nSleep     6.010e+00   6.010e+00   6.010e+00\n","category":"page"},{"location":"reference/helpers/#Helpers","page":"Helpers","title":"Helpers","text":"","category":"section"},{"location":"reference/helpers/#JaggedArray","page":"Helpers","title":"JaggedArray","text":"","category":"section"},{"location":"reference/helpers/","page":"Helpers","title":"Helpers","text":"Modules = [PartitionedArrays]\nPages = [\"jagged_array.jl\"]","category":"page"},{"location":"reference/helpers/#PartitionedArrays.GenericJaggedArray","page":"Helpers","title":"PartitionedArrays.GenericJaggedArray","text":"struct GenericJaggedArray{V,A,B}\n\nGeneralization of JaggedArray, where the fields data and ptrs are allowed to be any array-like object.\n\nProperties\n\ndata::A\nptrs::B\n\nSupertype hierarchy\n\nGenericJaggedArray{V,A,B} <: AbstractVector{V}\n\nGiven a::GenericJaggedArray, V is typeof(view(a.data,a.ptrs[i]:(a.ptrs[i+1]-1))).\n\n\n\n\n\n","category":"type"},{"location":"reference/helpers/#PartitionedArrays.GenericJaggedArray-Tuple{Any, Any}","page":"Helpers","title":"PartitionedArrays.GenericJaggedArray","text":"GenericJaggedArray(data,ptrs)\n\nCreate a GenericJaggedArray from the given data and ptrs fields. The resulting object stores references to the given vectors.\n\n\n\n\n\n","category":"method"},{"location":"reference/helpers/#PartitionedArrays.JaggedArray","page":"Helpers","title":"PartitionedArrays.JaggedArray","text":"struct JaggedArray{T,Ti}\n\nEfficient implementation of a vector of vectors. The inner vectors are stored one after the other in consecutive memory locations using an auxiliary vector data. The range of indices corresponding to each inner vector are encoded using a vector of integers ptrs.\n\nProperties\n\ndata::Vector{T}\nptrs::Vector{Ti}\n\nGiven a::JaggedArray, a.data contains the inner vectors. The i-th inner vector is stored in the range a.ptrs[i]:(a.ptrs[i+1]-1). The number of inner vectors (i.e. length(a)) is length(a.ptrs)-1. a[i] returns a view of a.data restricted to the range a.ptrs[i]:(a.ptrs[i+1]-1).\n\nSupertype hierarchy\n\nJaggedArray{T,Ti} <: AbstractVector{V}\n\nGiven a::JaggedArray, V is typeof(view(a.data,a.ptrs[i]:(a.ptrs[i+1]-1))).\n\n\n\n\n\n","category":"type"},{"location":"reference/helpers/#PartitionedArrays.JaggedArray-Union{Tuple{AbstractArray{<:AbstractArray{T}}}, Tuple{T}} where T","page":"Helpers","title":"PartitionedArrays.JaggedArray","text":"JaggedArray(a)\n\nCreate a JaggedArray object from the vector of vectors a. If a::JaggedArray, then a is returned. Otherwise, the contents of a are copied.\n\n\n\n\n\n","category":"method"},{"location":"reference/helpers/#PartitionedArrays.JaggedArray-Union{Tuple{Ti}, Tuple{T}, Tuple{Vector{T}, Vector{Ti}}} where {T, Ti}","page":"Helpers","title":"PartitionedArrays.JaggedArray","text":"JaggedArray(data::Vector,ptrs::Vector)\n\nCreate a JaggedArray from the given data and ptrs fields. The resulting object stores references to the given vectors.\n\n\n\n\n\n","category":"method"},{"location":"reference/helpers/#PartitionedArrays.getdata-Tuple{Any}","page":"Helpers","title":"PartitionedArrays.getdata","text":"getdata(a)\n\nReturn a.data\n\n\n\n\n\n","category":"method"},{"location":"reference/helpers/#PartitionedArrays.getptrs-Tuple{Any}","page":"Helpers","title":"PartitionedArrays.getptrs","text":"getptrs(a)\n\nReturn a.ptrs\n\n\n\n\n\n","category":"method"},{"location":"reference/helpers/#PartitionedArrays.jagged_array-Tuple{Any, Any}","page":"Helpers","title":"PartitionedArrays.jagged_array","text":"jagged_array(data,ptrs)\n\nCreate a JaggedArray or a GenericJaggedArray object depending on the type of data and ptrs. The returned object stores references to the given inputs.\n\n\n\n\n\n","category":"method"},{"location":"reference/helpers/#PartitionedArrays.length_to_ptrs!-Tuple{Any}","page":"Helpers","title":"PartitionedArrays.length_to_ptrs!","text":"length_to_ptrs!(ptrs)\n\nCompute the field ptrs of a JaggedArray. length(ptrs) should be the number of sub-vectors in the jagged array plus one. At input, ptrs[i+1] is the length of the i-th sub-vector. At output, ptrs[i]:(ptrs[i+1]-1) contains the range where the i-th sub-vector is stored in the data field of the jagged array.\n\n\n\n\n\n","category":"method"},{"location":"reference/helpers/#PartitionedArrays.rewind_ptrs!-Tuple{Any}","page":"Helpers","title":"PartitionedArrays.rewind_ptrs!","text":"rewind_ptrs(a)!\n\nSets a[i+1]=a[i] for i in (length(a)-1):-1:1 and then a[1] = one(eltype(a)).\n\n\n\n\n\n","category":"method"},{"location":"reference/helpers/#Sparse-utils","page":"Helpers","title":"Sparse utils","text":"","category":"section"},{"location":"reference/helpers/","page":"Helpers","title":"Helpers","text":"Modules = [PartitionedArrays]\nPages = [\"sparse_utils.jl\"]","category":"page"},{"location":"reference/helpers/#PartitionedArrays.compresscoo","page":"Helpers","title":"PartitionedArrays.compresscoo","text":"compresscoo(T,args...)\n\nLike sparse(args...), but generates a sparse matrix of type T.\n\n\n\n\n\n","category":"function"},{"location":"reference/helpers/#PartitionedArrays.indextype","page":"Helpers","title":"PartitionedArrays.indextype","text":"indextype(a)\n\nReturn the element type of the vector used to store the row or column indices in the sparse matrix a. \n\n\n\n\n\n","category":"function"},{"location":"reference/helpers/#PartitionedArrays.nzindex","page":"Helpers","title":"PartitionedArrays.nzindex","text":"nzindex(a,i,j)\n\nReturn the position in nonzeros(a) that stores the non zero value of a at row i and column j.\n\n\n\n\n\n","category":"function"},{"location":"reference/helpers/#PartitionedArrays.nziterator","page":"Helpers","title":"PartitionedArrays.nziterator","text":"for (i,j,v) in nziterator(a)\n...\nend\n\nIterate over the non zero entries of a returning the corresponding row i, column j and value v.\n\n\n\n\n\n","category":"function"},{"location":"examples/","page":"Examples","title":"Examples","text":"EditURL = \"../examples.jl\"","category":"page"},{"location":"examples/#Examples","page":"Examples","title":"Examples","text":"","category":"section"},{"location":"examples/","page":"Examples","title":"Examples","text":"note: Note\nThe following examples are run with the native Julia arrays for demo purposes. Substituting LinearIndices((np,)) by distribute_with_mpi(LinearIndices((np,))) will convert them to distributed drivers. To learn how to run the examples with MPI, see the Usage section.","category":"page"},{"location":"examples/#Hello,-world!","page":"Examples","title":"Hello, world!","text":"","category":"section"},{"location":"examples/","page":"Examples","title":"Examples","text":"using PartitionedArrays\nnp = 4\nranks = LinearIndices((np,))\nmap(ranks) do rank\n    println(\"Hello, world! I am proc $rank of $np.\")\nend;\nnothing #hide","category":"page"},{"location":"examples/#Collective-communication","page":"Examples","title":"Collective communication","text":"","category":"section"},{"location":"examples/","page":"Examples","title":"Examples","text":"The first rank generates an array of random integers in 1:30 and scatters it over all ranks. Each rank counts the number of even items in its part. Finally, the partial sums are reduced in the first rank.","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"The first rank generates the data to send.","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"using PartitionedArrays\nnp = 4\nload = 3\nn = load*np\nranks = LinearIndices((np,))\na_snd = map(ranks) do rank\n    if rank == 1\n        a = rand(1:30,n)\n        [ a[(1:load).+(i-1)*load] for i in 1:np ]\n    else\n        [ Int[] ]\n    end\nend","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"Note that only the first entry contains meaningful data in previous output.","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"a_rcv = scatter(a_snd,source=1)","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"After the scatter, all the parts have received their chunk. Now, we can count in parallel.","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"b_snd = map(ai->count(isodd,ai),a_rcv)","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"Finally we reduce the partial sums.","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"b_rcv = reduction(+,b_snd,init=0,destination=1)","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"Only the destination rank will receive the correct result.","category":"page"},{"location":"examples/#Point-to-point-communication","page":"Examples","title":"Point-to-point communication","text":"","category":"section"},{"location":"examples/","page":"Examples","title":"Examples","text":"Each rank generates some message (in this case an integer 10 times the current rank id). Each rank sends this data to the next rank. The last one sends it to the first, closing the circle. After repeating this exchange a number of times equal to the number of ranks, we check that we ended up with the original message.","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"First, each rank generates the ids of the neighbor to send data to.","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"using PartitionedArrays\nnp = 3\nranks = LinearIndices((np,))\nneigs_snd = map(ranks) do rank\n    if rank != np\n        [rank + 1]\n    else\n        [1]\n    end\nend","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"Now, generate the data we want to send","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"data_snd = map(ranks) do rank\n    [10*rank]\nend","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"Prepare, the point-to-point communication graph","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"graph = ExchangeGraph(neigs_snd)","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"Do the first exchange, and wait for the result to arrive","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"data_rcv = exchange(data_snd,graph) |> fetch","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"Do the second exchange and wait for the result to arrive","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"map(copy!,data_snd,data_rcv)\nexchange!(data_rcv,data_snd,graph) |> fetch","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"Do the last exchange","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"map(copy!,data_snd,data_rcv)\nexchange!(data_rcv,data_snd,graph) |> fetch","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"Check that we got the initial message","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"map(ranks,data_rcv) do rank,data_rcv\n    @assert data_rcv == [10*rank]\nend;\nnothing #hide","category":"page"},{"location":"examples/#Distributed-sparse-linear-solve","page":"Examples","title":"Distributed sparse linear solve","text":"","category":"section"},{"location":"examples/","page":"Examples","title":"Examples","text":"Solve the following linear system by distributing it over several parts.","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"beginpmatrix\n1   0   0   0   0 \n-1  2  -1   0   0 \n0  -1  2  -1   0 \n0   0  -1  2  -1 \n0   0   0   0   1\nendpmatrix\nbeginpmatrix\nu \nu \nu \nu \nu\nendpmatrix\n=\nbeginpmatrix\n 1 \n 0 \n 0 \n 0 \n-1\nendpmatrix","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"First generate the row partition","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"using PartitionedArrays\nusing IterativeSolvers\nusing LinearAlgebra\nnp = 3\nn = 5\nranks = LinearIndices((np,))\nrow_partition = uniform_partition(ranks,n)","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"Compute the rhs vector","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"IV = map(row_partition) do row_indices\n    I,V = Int[], Float64[]\n    for global_row in local_to_global(row_indices)\n        if global_row == 1\n            v = 1.0\n        elseif global_row == n\n            v = -1.0\n        else\n            continue\n        end\n        push!(I,global_row)\n        push!(V,v)\n    end\n    I,V\nend\nII,VV = tuple_of_arrays(IV)\nb = pvector(II,VV,row_partition) |> fetch","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"Compute the system matrix","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"IJV = map(row_partition) do row_indices\n    I,J,V = Int[], Int[], Float64[]\n    for global_row in local_to_global(row_indices)\n        if global_row in (1,n)\n            push!(I,global_row)\n            push!(J,global_row)\n            push!(V,1.0)\n        else\n            push!(I,global_row)\n            push!(J,global_row-1)\n            push!(V,-1.0)\n            push!(I,global_row)\n            push!(J,global_row)\n            push!(V,2.0)\n            push!(I,global_row)\n            push!(J,global_row+1)\n            push!(V,-1.0)\n        end\n    end\n    I,J,V\nend\nI,J,V = tuple_of_arrays(IJV)\ncol_partition = row_partition\nA = psparse(I,J,V,row_partition,col_partition) |> fetch","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"Generate an initial guess that fulfills the boundary conditions. Solve and check the result","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"x = similar(b,axes(A,2))\nx .= b\nIterativeSolvers.cg!(x,A,b)\nr = A*x - b\nnorm(r)","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"Imagine, that we want to create the system several times (e.g., in a nonlinear solver). We can use the key-word argument reuse to enable efficient re-construction of the matrix/vector.","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"b,cacheb = pvector(II,VV,row_partition,reuse=true) |> fetch\nA,cacheA= psparse(I,J,V,row_partition,col_partition,reuse=true) |> fetch;\nnothing #hide","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"Now modify the values. For example:","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"V = map(i->2*i,V)","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"VV = map(i->2*i,VV)","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"Update the matrix and the vector accordingly:","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"pvector!(b,VV,cacheb) |> wait\npsparse!(A,V,cacheA) |> wait","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"The solution should be the same as before in this case.","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"r = A*x - b\nnorm(r)","category":"page"},{"location":"examples/#Distributed-algebraic-multigrid-(AMG)","page":"Examples","title":"Distributed algebraic multigrid (AMG)","text":"","category":"section"},{"location":"examples/","page":"Examples","title":"Examples","text":"So far, we have use a conjugate gradient method to solve the linear system. However, this is approach does not scale well to larger problems and one needs to consider a preconditioner. A distributed algebraic multigrid (AMG) preconditioner is available in PartitonedSolvers, an extension package of PartitionedArrays that provides parallel solvers for systems built with matrices and vectors from PartitionedArrays.","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"First, let us solve a larger problem without a preconditioner. To this end we use function laplace_matrix that builds a Laplace matrix of arbitrary size.","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"using PartitionedArrays: laplace_matrix\n\nnodes_per_dir = (40,40,40)\nparts_per_dir = (2,2,1)\nnparts = prod(parts_per_dir)\nparts = LinearIndices((nparts,))\nA = laplace_matrix(nodes_per_dir,parts_per_dir,parts)\nx_exact = pones(partition(axes(A,2)))\nb = A*x_exact","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"Now, we have a matrix and a rhs vector. Let us solve the system:","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"x = similar(b,axes(A,2))\nx .= 0\n_, history = IterativeSolvers.cg!(x,A,b;log=true)\nhistory","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"Now solve the system while using an AMG preconditioner.","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"using PartitionedSolvers: amg, preconditioner\n\nx .= 0\nPl = preconditioner(amg(),x,A,b)\n_, history = IterativeSolvers.cg!(x,A,b;Pl,log=true)\nhistory","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"This page was generated using Literate.jl.","category":"page"},{"location":"#PartitionedArrays.jl","page":"Introduction","title":"PartitionedArrays.jl","text":"","category":"section"},{"location":"","page":"Introduction","title":"Introduction","text":"Welcome to the documentation for PartitionedArrays.jl!","category":"page"},{"location":"#What","page":"Introduction","title":"What","text":"","category":"section"},{"location":"","page":"Introduction","title":"Introduction","text":"This package provides distributed (a.k.a. partitioned) vectors and sparse matrices like the ones needed in distributed finite differences, finite volumes, or finite element computations. Packages such GridapDistributed have shown weak and strong scaling up to tens of thousands of CPU cores in the distributed assembly of sparse linear systems when using PartitionedArrays as their distributed linear algebra back-end. See this publication for further details:","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"Santiago Badia, Alberto F. Martn, and Francesc Verdugo (2022). \"GridapDistributed: a massively parallel finite element toolbox in Julia\". Journal of Open Source Software, 7(74), 4157.  doi: 10.21105/joss.04157.","category":"page"},{"location":"#Why","page":"Introduction","title":"Why","text":"","category":"section"},{"location":"","page":"Introduction","title":"Introduction","text":"The main objective of this package is to avoid to interface directly with MPI or MPI-based libraries when prototyping and debugging distributed parallel codes. MPI-based applications are executed in batch mode with commands like mpiexec -np 4 julia input.jl, which break the Julia development workflow. In particular, one starts a fresh Julia session at each run, making difficult to reuse compiled code between runs. In addition, packages like Revise and Debugger are also difficult to use in combination with MPI computations on several processes.","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"To overcome these limitations, PartitionedArrays considers a data-oriented programming model that allows one to write distributed algorithms in a generic way, independent from the message passing back-end used to run them.  MPI is one of the possible back-ends available in PartitionedArrays, used to deploy large computations on computational clusters. However, one can also use other back-ends that are able to run on standard serial Julia sessions, which allows one to use the standard Julia workflow to develop and debug complex codes in an effective way.","category":"page"}]
}
